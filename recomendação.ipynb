{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMD3t/ZjjnReGMZ7YNEpkK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thales20266/Projeto-Transfer-Learning/blob/main/recomenda%C3%A7%C3%A3o.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUwkU2bIEdvi"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "image_recommender.py\n",
        "\n",
        "Um script único que cobre:\n",
        "- Treinamento por transferência (ResNet18)\n",
        "- Extração de embeddings\n",
        "- Busca por imagens similares localmente (NearestNeighbors)\n",
        "- Integração opcional com Bing Image Search para buscar imagens na web\n",
        "\n",
        "Requisitos:\n",
        "pip install torch torchvision scikit-learn pillow requests tqdm\n",
        "\n",
        "Uso (exemplos):\n",
        "# Treinar\n",
        "python image_recommender.py --mode train --data_dir ./data/train --model_out model.pth --epochs 5\n",
        "\n",
        "# Construir índice de embeddings a partir de pasta de imagens (dataset de referência)\n",
        "python image_recommender.py --mode build_index --ref_images ./data/ref_images --index_out index.npz --model_in model.pth\n",
        "\n",
        "# Recomendar localmente para uma imagem de consulta\n",
        "python image_recommender.py --mode recommend_local --model_in model.pth --index_in index.npz --query_image query.jpg --top_k 5\n",
        "\n",
        "# Recomendar de imagens web (necessita BING_API_KEY)\n",
        "python image_recommender.py --mode recommend_web --model_in model.pth --query_image query.jpg --bing_key YOUR_KEY --top_k 5\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets, models\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# ---------------------\n",
        "# Config / Transforms\n",
        "# ---------------------\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# ---------------------\n",
        "# Model helpers\n",
        "# ---------------------\n",
        "def build_model(num_classes: int, pretrained=True, embedding_dim=512):\n",
        "    \"\"\"\n",
        "    Retorna um modelo com classificação e possibilidade de extrair embeddings.\n",
        "    Utiliza ResNet18 e altera a última camada.\n",
        "    \"\"\"\n",
        "    model = models.resnet18(pretrained=pretrained)\n",
        "    # ResNet18 final fc in_features\n",
        "    in_f = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_f, num_classes)\n",
        "    return model\n",
        "\n",
        "class EmbeddingModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper para extrair embeddings (features antes da última FC)\n",
        "    \"\"\"\n",
        "    def __init__(self, base_model: nn.Module):\n",
        "        super().__init__()\n",
        "        # copy all layers except final fc\n",
        "        # For resnet, take everything up to avgpool and fc separately\n",
        "        self.features = nn.Sequential(\n",
        "            base_model.conv1,\n",
        "            base_model.bn1,\n",
        "            base_model.relu,\n",
        "            base_model.maxpool,\n",
        "            base_model.layer1,\n",
        "            base_model.layer2,\n",
        "            base_model.layer3,\n",
        "            base_model.layer4,\n",
        "            base_model.avgpool,  # yields (batch, in_f, 1, 1)\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        # final fc exists separately but we won't include it here\n",
        "        self.final_in_features = base_model.fc.in_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.flatten(x)  # (batch, final_in_features)\n",
        "        return x\n",
        "\n",
        "# ---------------------\n",
        "# Training\n",
        "# ---------------------\n",
        "def train(\n",
        "    data_dir: str,\n",
        "    model_out: str,\n",
        "    epochs: int = 5,\n",
        "    lr: float = 1e-3,\n",
        "    weight_decay: float = 1e-4,\n",
        "):\n",
        "    \"\"\"\n",
        "    Treina um classificador usando ImageFolder em data_dir (com subpastas por classe)\n",
        "    Salva o modelo em model_out.\n",
        "    \"\"\"\n",
        "    train_dataset = datasets.ImageFolder(root=data_dir, transform=transform_train)\n",
        "    num_classes = len(train_dataset.classes)\n",
        "    print(f\"Found {len(train_dataset)} images, {num_classes} classes.\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = build_model(num_classes=num_classes, pretrained=True).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for imgs, labels in pbar:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            pbar.set_postfix(loss=running_loss / total, acc=100. * correct / total)\n",
        "\n",
        "        scheduler.step()\n",
        "        epoch_loss = running_loss / len(train_dataset)\n",
        "        epoch_acc = 100. * correct / len(train_dataset)\n",
        "        print(f\"Epoch {epoch+1} finished: loss={epoch_loss:.4f}, acc={epoch_acc:.2f}%\")\n",
        "\n",
        "    # Save model + class_to_idx for inference\n",
        "    state = {\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"classes\": train_dataset.classes,\n",
        "    }\n",
        "    torch.save(state, model_out)\n",
        "    print(f\"Model saved to {model_out}\")\n",
        "\n",
        "# ---------------------\n",
        "# Embedding & Index\n",
        "# ---------------------\n",
        "def load_model_for_embedding(model_path: str, device=DEVICE):\n",
        "    \"\"\"\n",
        "    Carrega o modelo salvo e retorna EmbeddingModel and classification model (if available)\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    # We need a base resnet18 to reconstruct embedding extractor\n",
        "    # If checkpoint doesn't have num_classes stored, we'll try to infer\n",
        "    # Create temp model to load weights\n",
        "    # This assumes the original was resnet18\n",
        "    # Build dummy classification model (we won't use its fc directly for embeddings)\n",
        "    # For safety, try to infer number of classes from fc weight shape\n",
        "    # If missing, fallback to 1000 and ignore mismatch\n",
        "    # We'll construct model and load partial state\n",
        "    # Simple approach: instantiate resnet18 with 1000 classes then try to load state (ignore mismatch for fc)\n",
        "    base = models.resnet18(pretrained=False)\n",
        "    # try to load state dict permissively\n",
        "    sd = checkpoint.get(\"model_state\", checkpoint)\n",
        "    base_state = base.state_dict()\n",
        "    # filter sd keys to those in base_state\n",
        "    filtered = {k: v for k, v in sd.items() if k in base_state and v.shape == base_state[k].shape}\n",
        "    base_state.update(filtered)\n",
        "    base.load_state_dict(base_state)\n",
        "    emb_model = EmbeddingModel(base).to(device)\n",
        "    emb_model.eval()\n",
        "    # We may also want the classifier object with class names\n",
        "    classes = checkpoint.get(\"classes\", None)\n",
        "    return emb_model, classes\n",
        "\n",
        "def image_to_tensor(img_path: str, transform=transform_eval):\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    return transform(img).unsqueeze(0)  # batch dim\n",
        "\n",
        "def compute_embeddings_for_folder(folder: str, model: nn.Module, batch_size=32) -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"\n",
        "    Percorre as imagens em `folder`, calcula embeddings e retorna (embeddings, list_of_paths).\n",
        "    \"\"\"\n",
        "    image_paths = []\n",
        "    for ext in (\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\"):\n",
        "        image_paths.extend(Path(folder).rglob(f\"*{ext}\"))\n",
        "    image_paths = sorted([str(p) for p in image_paths])\n",
        "    print(f\"Found {len(image_paths)} images in reference folder.\")\n",
        "\n",
        "    emb_list = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(image_paths), batch_size):\n",
        "            batch_paths = image_paths[i:i+batch_size]\n",
        "            tensors = [image_to_tensor(p) for p in batch_paths]\n",
        "            batch = torch.cat(tensors, dim=0).to(next(model.parameters()).device)\n",
        "            feats = model(batch)  # (B, D)\n",
        "            feats_np = feats.cpu().numpy()\n",
        "            emb_list.append(feats_np)\n",
        "    embeddings = np.vstack(emb_list) if len(emb_list) else np.zeros((0, model.final_in_features))\n",
        "    return embeddings, image_paths\n",
        "\n",
        "def build_and_save_index(ref_folder: str, model_path: str, index_out: str):\n",
        "    emb_model, _ = load_model_for_embedding(model_path)\n",
        "    embeddings, paths = compute_embeddings_for_folder(ref_folder, emb_model, batch_size=BATCH_SIZE)\n",
        "    # Fit NearestNeighbors\n",
        "    nbrs = NearestNeighbors(n_neighbors=50, algorithm=\"auto\", metric=\"cosine\")\n",
        "    if len(embeddings) == 0:\n",
        "        print(\"No embeddings found — abort.\")\n",
        "        return\n",
        "    nbrs.fit(embeddings)\n",
        "    # Save embeddings and mapping\n",
        "    np.savez_compressed(index_out, embeddings=embeddings, paths=np.array(paths))\n",
        "    # We can't easily pickle sklearn's NearestNeighbors robustly across environments; we'll re-fit on load from embeddings\n",
        "    print(f\"Index saved to {index_out} (embeddings shape {embeddings.shape})\")\n",
        "\n",
        "def load_index(index_in: str):\n",
        "    data = np.load(index_in, allow_pickle=True)\n",
        "    embeddings = data[\"embeddings\"]\n",
        "    paths = data[\"paths\"].tolist()\n",
        "    nbrs = NearestNeighbors(n_neighbors=10, algorithm=\"auto\", metric=\"cosine\")\n",
        "    nbrs.fit(embeddings)\n",
        "    return nbrs, embeddings, paths\n",
        "\n",
        "def recommend_local(query_image_path: str, model_path: str, index_in: str, top_k: int = 5):\n",
        "    emb_model, _ = load_model_for_embedding(model_path)\n",
        "    nbrs, embeddings, paths = load_index(index_in)\n",
        "    # query embedding\n",
        "    with torch.no_grad():\n",
        "        t = image_to_tensor(query_image_path).to(next(emb_model.parameters()).device)\n",
        "        q_emb = emb_model(t).cpu().numpy()\n",
        "    dists, idxs = nbrs.kneighbors(q_emb, n_neighbors=top_k)\n",
        "    results = []\n",
        "    for dist, idx in zip(dists[0], idxs[0]):\n",
        "        results.append({\"path\": paths[idx], \"score\": float(1.0 - dist)})  # convert cosine distance to similarity-ish\n",
        "    return results\n",
        "\n",
        "# ---------------------\n",
        "# Web image search (Bing) + comparison\n",
        "# ---------------------\n",
        "def bing_image_search(query: str, bing_api_key: str, count: int = 10) -> List[str]:\n",
        "    \"\"\"\n",
        "    Usa Bing Image Search v7 REST API para buscar imagens.\n",
        "    Retorna lista de image urls.\n",
        "    Você precisa configurar uma chave: azure/cognitive services Bing Image Search.\n",
        "    Docs example: https://learn.microsoft.com/en-us/bing/search-apis/bing-image-search/overview\n",
        "    \"\"\"\n",
        "    assert bing_api_key, \"bing_api_key is required\"\n",
        "    endpoint = \"https://api.bing.microsoft.com/v7.0/images/search\"\n",
        "    headers = {\"Ocp-Apim-Subscription-Key\": bing_api_key}\n",
        "    params = {\"q\": query, \"count\": count}\n",
        "    resp = requests.get(endpoint, headers=headers, params=params, timeout=10)\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    urls = []\n",
        "    for v in data.get(\"value\", []):\n",
        "        content_url = v.get(\"contentUrl\")\n",
        "        if content_url:\n",
        "            urls.append(content_url)\n",
        "    return urls\n",
        "\n",
        "def download_image_to_pil(url: str, timeout=8):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=timeout)\n",
        "        r.raise_for_status()\n",
        "        return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        # ignore failed downloads\n",
        "        return None\n",
        "\n",
        "def recommend_web(query_image_path: str, model_path: str, bing_key: str, top_k: int = 5, bing_count: int = 20):\n",
        "    \"\"\"\n",
        "    Busca imagens na web (Bing) com base em classes ou texto gerado pelo classificador.\n",
        "    Simples approach: usa filename or predicted class as query.\n",
        "    Melhorias possíveis: usar um modelo captioning para gerar query rica.\n",
        "    \"\"\"\n",
        "    # Load classification model to predict a class label for query image\n",
        "    checkpoint = torch.load(model_path, map_location=DEVICE)\n",
        "    # We don't have a full class->index mapping routine here; we will try to create a resnet and load matching weights (like before)\n",
        "    base = models.resnet18(pretrained=False)\n",
        "    sd = checkpoint.get(\"model_state\", checkpoint)\n",
        "    base_state = base.state_dict()\n",
        "    filtered = {k: v for k, v in sd.items() if k in base_state and v.shape == base_state[k].shape}\n",
        "    base_state.update(filtered)\n",
        "    base.load_state_dict(base_state)\n",
        "    # Attempt to build a classifier for prediction\n",
        "    classifier = base.to(DEVICE)\n",
        "    classifier.eval()\n",
        "    classes = checkpoint.get(\"classes\", None)\n",
        "\n",
        "    # Preprocess and predict\n",
        "    with torch.no_grad():\n",
        "        t = image_to_tensor(query_image_path).to(DEVICE)\n",
        "        out = classifier(t)\n",
        "        pred_idx = int(out.argmax(dim=1).item())\n",
        "        pred_label = classes[pred_idx] if classes is not None and pred_idx < len(classes) else str(pred_idx)\n",
        "\n",
        "    # Use label as Bing query. Alternative: generate caption with an image-to-text model if available.\n",
        "    query = pred_label\n",
        "    print(f\"Predicted label for query image: {pred_label}. Using it to search Bing for similar images.\")\n",
        "\n",
        "    urls = bing_image_search(query, bing_api_key=bing_key, count=bing_count)\n",
        "    print(f\"Got {len(urls)} candidate urls from Bing. Downloading and embedding (may take a while).\")\n",
        "\n",
        "    emb_model, _ = load_model_for_embedding(model_path)\n",
        "    query_emb = emb_model(image_to_tensor(query_image_path).to(next(emb_model.parameters()).device)).cpu().numpy()\n",
        "\n",
        "    # Download images and compute embeddings incrementally\n",
        "    candidates = []\n",
        "    for url in urls:\n",
        "        pil = download_image_to_pil(url)\n",
        "        if pil is None:\n",
        "            continue\n",
        "        # transform to tensor\n",
        "        img_t = transform_eval(pil).unsqueeze(0).to(next(emb_model.parameters()).device)\n",
        "        with torch.no_grad():\n",
        "            emb = emb_model(img_t).cpu().numpy()\n",
        "        # compute cosine similarity\n",
        "        # cosine similarity = 1 - cosine_distance\n",
        "        dot = np.dot(query_emb, emb.T)[0][0]\n",
        "        q_norm = np.linalg.norm(query_emb)\n",
        "        e_norm = np.linalg.norm(emb)\n",
        "        if q_norm == 0 or e_norm == 0:\n",
        "            sim = 0.0\n",
        "        else:\n",
        "            sim = float(dot / (q_norm * e_norm))\n",
        "        candidates.append({\"url\": url, \"similarity\": sim})\n",
        "    # sort by similarity desc\n",
        "    candidates = sorted(candidates, key=lambda x: x[\"similarity\"], reverse=True)[:top_k]\n",
        "    return {\"predicted_label\": pred_label, \"results\": candidates}\n",
        "\n",
        "# ---------------------\n",
        "# CLI\n",
        "# ---------------------\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--mode\", choices=[\"train\", \"build_index\", \"recommend_local\", \"recommend_web\"], required=True)\n",
        "    p.add_argument(\"--data_dir\")\n",
        "    p.add_argument(\"--model_out\")\n",
        "    p.add_argument(\"--model_in\")\n",
        "    p.add_argument(\"--epochs\", type=int, default=5)\n",
        "    p.add_argument(\"--ref_images\")\n",
        "    p.add_argument(\"--index_out\")\n",
        "    p.add_argument(\"--index_in\")\n",
        "    p.add_argument(\"--query_image\")\n",
        "    p.add_argument(\"--top_k\", type=int, default=5)\n",
        "    p.add_argument(\"--bing_key\", help=\"Bing API key for web image search\")\n",
        "    return p.parse_args()\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    if args.mode == \"train\":\n",
        "        assert args.data_dir and args.model_out\n",
        "        train(args.data_dir, args.model_out, epochs=args.epochs)\n",
        "    elif args.mode == \"build_index\":\n",
        "        assert args.ref_images and args.model_in and args.index_out\n",
        "        build_and_save_index(args.ref_images, args.model_in, args.index_out)\n",
        "    elif args.mode == \"recommend_local\":\n",
        "        assert args.model_in and args.index_in and args.query_image\n",
        "        results = recommend_local(args.query_image, args.model_in, args.index_in, top_k=args.top_k)\n",
        "        print(\"Top local recommendations:\")\n",
        "        print(json.dumps(results, indent=2))\n",
        "    elif args.mode == \"recommend_web\":\n",
        "        assert args.model_in and args.query_image and args.bing_key\n",
        "        results = recommend_web(args.query_image, args.model_in, args.bing_key, top_k=args.top_k)\n",
        "        print(\"Top web recommendations:\")\n",
        "        print(json.dumps(results, indent=2))\n",
        "    else:\n",
        "        print(\"Unknown mode\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}