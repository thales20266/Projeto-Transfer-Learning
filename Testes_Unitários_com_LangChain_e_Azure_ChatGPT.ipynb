{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPca2FGBNcMeTv+wcQJSt7J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thales20266/Projeto-Transfer-Learning/blob/main/Testes_Unit%C3%A1rios_com_LangChain_e_Azure_ChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GI3-x1j0-1WK",
        "outputId": "402e04e8-e592-4aea-a69e-409127aeeaec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated f-string literal (detected at line 42) (ipython-input-1494990489.py, line 42)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1494990489.py\"\u001b[0;36m, line \u001b[0;32m42\u001b[0m\n\u001b[0;31m    print(f\"Testes\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated f-string literal (detected at line 42)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Inicializando o modelo Azure OpenAI\n",
        "llm = AzureChatOpenAI(\n",
        "    deployment_name=os.getenv(\"AZURE_DEPLOYMENT_NAME\"),\n",
        "    openai_api_version=\"2023-07-01-preview\",\n",
        "    model=\"gpt-35-turbo\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Função que gera o prompt\n",
        "def generate_test_code(python_code: str, file_name: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "    Gere um arquivo de testes pytest para o seguinte código Python:\n",
        "\n",
        "    {python_code}\n",
        "\n",
        "    Regras:\n",
        "    - Importar pytest na primeira linha\n",
        "    - Criar funções de teste com def test_<nome>()\n",
        "    - Cobrir casos de sucesso e falha\n",
        "    - Retornar apenas o código do arquivo de teste\n",
        "    \"\"\"\n",
        "    chain = LLMChain(llm=llm, prompt=ChatPromptTemplate.from_template(prompt))\n",
        "    return chain.run(python_code)\n",
        "\n",
        "# Função principal\n",
        "def main():\n",
        "    input_file = \"examples/math_funcs.py\"\n",
        "    with open(input_file, \"r\") as f:\n",
        "        code = f.read()\n",
        "\n",
        "    test_code = generate_test_code(code, input_file)\n",
        "\n",
        "    output_file = f\"tests/test_{os.path.basename(input_file)}\"\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(test_code)\n",
        "\n",
        "    print(f\"Testes"
      ]
    }
  ]
}